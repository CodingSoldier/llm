{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T14:38:32.324927Z",
     "start_time": "2025-10-01T14:38:32.311837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"ä½ æ˜¯ä¸€ä¸ª{name}ï¼Œå¸®æˆ‘èµ·ä¸€ä¸ªå…·æœ‰{county}ç‰¹è‰²çš„{sex}åå­—\")\n",
    "prompts = prompt.format(name=\"ç®—å‘½å¤§å¸ˆ\", county=\"æ³•å›½\", sex=\"å¥³å­©\")\n",
    "print(prompts)"
   ],
   "id": "daa561585952cb27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯ä¸€ä¸ªç®—å‘½å¤§å¸ˆï¼Œå¸®æˆ‘èµ·ä¸€ä¸ªå…·æœ‰æ³•å›½ç‰¹è‰²çš„å¥³å­©åå­—\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T14:44:25.195667Z",
     "start_time": "2025-10-01T14:44:25.157532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆï¼Œä½ çš„åå­—å«\"),\n",
    "    (\"human\", \"ä½ å¥½{name},ä½ æ„Ÿè§‰å¦‚ä½•ï¼Ÿ\"),\n",
    "    (\"ai\", \"ä½ å¥½ï¼æˆ‘çŠ¶æ€éžå¸¸å¥½!\"),\n",
    "    (\"human\", \"ä½ å«ä»€ä¹ˆåå­—å‘¢?\"),\n",
    "    (\"ai\", \"ä½ å¥½ï¼æˆ‘å«{name}\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "chats = chat_template.format_messages(name=\"é™ˆå¤§å¸ˆ\", user_input=\"ä½ çš„çˆ¸çˆ¸æ˜¯è°ï¼Ÿ\")\n",
    "print(chats)"
   ],
   "id": "c7c6093671100da7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆï¼Œä½ çš„åå­—å«', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¥½é™ˆå¤§å¸ˆ,ä½ æ„Ÿè§‰å¦‚ä½•ï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='ä½ å¥½ï¼æˆ‘çŠ¶æ€éžå¸¸å¥½!', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å«ä»€ä¹ˆåå­—å‘¢?', additional_kwargs={}, response_metadata={}), AIMessage(content='ä½ å¥½ï¼æˆ‘å«é™ˆå¤§å¸ˆ', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ çš„çˆ¸çˆ¸æ˜¯è°ï¼Ÿ', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T12:31:59.931739Z",
     "start_time": "2025-10-02T12:31:58.926551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªå¾ˆåŽ‰å®³çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "result = prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n",
    "print(result)"
   ],
   "id": "fe4de8aef01838df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªå¾ˆåŽ‰å®³çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T12:41:38.280555Z",
     "start_time": "2025-10-02T12:41:38.274221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "sy = SystemMessage(\n",
    "    content = \"ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ\",\n",
    "    additional_kwargs = {\"å¤§å¸ˆåå­—\": \"ç‹—è›‹\"}\n",
    ")\n",
    "hu = HumanMessage(\n",
    "    content=\"è¯·é—®å¤§å¸ˆå«ä»€ä¹ˆï¼Ÿ\"\n",
    ")\n",
    "ai = AIMessage(\n",
    "    content=\"æˆ‘å«ä½ çˆ¹\"\n",
    ")\n",
    "print([sy, hu, ai])"
   ],
   "id": "abc6789c6111c31f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ', additional_kwargs={'å¤§å¸ˆåå­—': 'ç‹—è›‹'}, response_metadata={}), HumanMessage(content='è¯·é—®å¤§å¸ˆå«ä»€ä¹ˆï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘å«ä½ çˆ¹', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:18:38.640634Z",
     "start_time": "2025-10-02T13:18:33.256700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##å‡½æ•°å¤§å¸ˆï¼šæ ¹æ®å‡½æ•°åç§°ï¼ŒæŸ¥æ‰¾å‡½æ•°ä»£ç ï¼Œå¹¶ç»™å‡ºä¸­æ–‡çš„ä»£ç è¯´æ˜Ž\n",
    "# Function Master: Given a function name, find the function code and provide a Chinese code explanation\n",
    "\n",
    "\n",
    "from langchain_core.prompts import StringPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°ä½œä¸ºç¤ºä¾‹æ•ˆæžœ\n",
    "# Define a simple function as an example\n",
    "def hello_world(abc):\n",
    "    print(\"Hello, world!\")\n",
    "    return abc\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "ä½ æ˜¯ä¸€ä¸ªéžå¸¸æœ‰ç»éªŒå’Œå¤©èµ‹çš„ç¨‹åºå‘˜ï¼ŒçŽ°åœ¨ç»™ä½ å¦‚ä¸‹å‡½æ•°åç§°ï¼Œä½ ä¼šæŒ‰ç…§å¦‚ä¸‹æ ¼å¼ï¼Œè¾“å‡ºè¿™æ®µä»£ç çš„åç§°ã€æºä»£ç ã€ä¸­æ–‡è§£é‡Šã€‚\n",
    "å‡½æ•°åç§°: {function_name}\n",
    "æºä»£ç :\n",
    "{source_code}\n",
    "ä»£ç è§£é‡Š:\n",
    "\"\"\"\n",
    "\n",
    "import inspect\n",
    "\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    #èŽ·å¾—æºä»£ç \n",
    "    # Get the source code\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "#è‡ªå®šä¹‰çš„æ¨¡æ¿class\n",
    "# Custom template class\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # èŽ·å¾—æºä»£ç \n",
    "        # Get the source code\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # ç”Ÿæˆæç¤ºè¯æ¨¡æ¿\n",
    "        # Generate the prompt template\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_world)\n",
    "\n",
    "print(pm)\n",
    "\n",
    "#å’ŒLLMè¿žæŽ¥èµ·æ¥\n",
    "# Connect to LLM\n",
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\")\n",
    ")\n",
    "msg = llm.invoke(pm)\n",
    "print(msg)"
   ],
   "id": "18f86cbe2f11556",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯ä¸€ä¸ªéžå¸¸æœ‰ç»éªŒå’Œå¤©èµ‹çš„ç¨‹åºå‘˜ï¼ŒçŽ°åœ¨ç»™ä½ å¦‚ä¸‹å‡½æ•°åç§°ï¼Œä½ ä¼šæŒ‰ç…§å¦‚ä¸‹æ ¼å¼ï¼Œè¾“å‡ºè¿™æ®µä»£ç çš„åç§°ã€æºä»£ç ã€ä¸­æ–‡è§£é‡Šã€‚\n",
      "å‡½æ•°åç§°: hello_world\n",
      "æºä»£ç :\n",
      "def hello_world(abc):\n",
      "    print(\"Hello, world!\")\n",
      "    return abc\n",
      "\n",
      "ä»£ç è§£é‡Š:\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'ip:port failed: json: cannot unmarshal array into Go struct field ip:port of type string (request id: 202510022118373120759637495192)', 'type': 'shell_api_error', 'param': '', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 68\u001B[39m\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ChatOpenAI\n\u001B[32m     62\u001B[39m llm = OpenAI(\n\u001B[32m     63\u001B[39m     model=\u001B[33m\"\u001B[39m\u001B[33mgpt-3.5-turbo-instruct\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     64\u001B[39m     temperature=\u001B[32m0\u001B[39m,\n\u001B[32m     65\u001B[39m     openai_api_key=os.getenv(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     66\u001B[39m     openai_api_base=os.getenv(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_BASE\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     67\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m msg = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[38;5;28mprint\u001B[39m(msg)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:390\u001B[39m, in \u001B[36mBaseLLM.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    380\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    381\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    386\u001B[39m     **kwargs: Any,\n\u001B[32m    387\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    388\u001B[39m     config = ensure_config(config)\n\u001B[32m    389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m390\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    391\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    394\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    395\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    400\u001B[39m         .generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m    401\u001B[39m         .text\n\u001B[32m    402\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:789\u001B[39m, in \u001B[36mBaseLLM.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    780\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    781\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    782\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    786\u001B[39m     **kwargs: Any,\n\u001B[32m    787\u001B[39m ) -> LLMResult:\n\u001B[32m    788\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1000\u001B[39m, in \u001B[36mBaseLLM.generate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    985\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m    986\u001B[39m     run_managers = [\n\u001B[32m    987\u001B[39m         callback_manager.on_llm_start(\n\u001B[32m    988\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m    998\u001B[39m         )\n\u001B[32m    999\u001B[39m     ]\n\u001B[32m-> \u001B[39m\u001B[32m1000\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) > \u001B[32m0\u001B[39m:\n\u001B[32m   1008\u001B[39m     run_managers = [\n\u001B[32m   1009\u001B[39m         callback_managers[idx].on_llm_start(\n\u001B[32m   1010\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1017\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m missing_prompt_idxs\n\u001B[32m   1018\u001B[39m     ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:815\u001B[39m, in \u001B[36mBaseLLM._generate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m    804\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate_helper\u001B[39m(\n\u001B[32m    805\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    806\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    811\u001B[39m     **kwargs: Any,\n\u001B[32m    812\u001B[39m ) -> LLMResult:\n\u001B[32m    813\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    814\u001B[39m         output = (\n\u001B[32m--> \u001B[39m\u001B[32m815\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    816\u001B[39m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    817\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    818\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[32m    819\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    820\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    821\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    822\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    823\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._generate(prompts, stop=stop)\n\u001B[32m    824\u001B[39m         )\n\u001B[32m    825\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    826\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\langchain_openai\\llms\\base.py:446\u001B[39m, in \u001B[36mBaseOpenAI._generate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    430\u001B[39m     choices.append(\n\u001B[32m    431\u001B[39m         {\n\u001B[32m    432\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m\"\u001B[39m: generation.text,\n\u001B[32m   (...)\u001B[39m\u001B[32m    443\u001B[39m         }\n\u001B[32m    444\u001B[39m     )\n\u001B[32m    445\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m446\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_prompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    447\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    448\u001B[39m         \u001B[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001B[39;00m\n\u001B[32m    449\u001B[39m         \u001B[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001B[39;00m\n\u001B[32m    450\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    284\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    285\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\openai\\resources\\completions.py:541\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    512\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mprompt\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mprompt\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    513\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m    514\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    539\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m    540\u001B[39m ) -> Completion | Stream[Completion]:\n\u001B[32m--> \u001B[39m\u001B[32m541\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    542\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    543\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    546\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprompt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    547\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbest_of\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    548\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mecho\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mecho\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    549\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    552\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    553\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msuffix\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    560\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    562\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    563\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    564\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[32m    565\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[32m    566\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    568\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    569\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    570\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    571\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1245\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1246\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1247\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1254\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1255\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1256\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1257\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1258\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1259\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\github\\llm\\02langchain\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1044\u001B[39m             err.response.read()\n\u001B[32m   1046\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mBadRequestError\u001B[39m: Error code: 400 - {'error': {'message': 'ip:port failed: json: cannot unmarshal array into Go struct field ip:port of type string (request id: 202510022118373120759637495192)', 'type': 'shell_api_error', 'param': '', 'code': None}}"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T11:51:51.447720Z",
     "start_time": "2025-10-03T11:51:47.857746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# zeroshotä¼šå¯¼è‡´ä½Žè´¨é‡å›žç­”\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(\n",
    "model=\"gpt-4o-mini\",\n",
    "temperature=0.0,\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\"),\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "res = model.invoke(\"What is 2 ðŸ¦œ 9?\")\n",
    "print(\"ä½Žè´¨é‡å›žç­”\")\n",
    "print(res)\n",
    "\n",
    "#å¢žåŠ ç¤ºä¾‹\n",
    "#ä½¿ç”¨FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "#å¢žåŠ ç¤ºä¾‹ç»„\n",
    "examples = [\n",
    "    {\"input\": \"2 ðŸ¦œ 2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2 ðŸ¦œ 3\", \"output\": \"5\"},\n",
    "]\n",
    "#æž„é€ æç¤ºè¯æ¨¡æ¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "#ç»„åˆç¤ºä¾‹ä¸Žæç¤ºè¯\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "#æ‰“å°æç¤ºè¯æ¨¡æ¿\n",
    "print(few_shot_prompt.invoke({}).to_messages())\n",
    "\n",
    "## æœ€ç»ˆæç¤ºè¯\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€ä½ç¥žå¥‡çš„æ•°å­¦å¥‡æ‰\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "##é‡æ–°æé—®\n",
    "chain = final_prompt | model\n",
    "result = chain.invoke({\"input\": \"What is 2 ðŸ¦œ 9?\"})\n",
    "print(\"ä½¿ç”¨few_shotåŽçš„é«˜è´¨é‡å›žç­”\")\n",
    "print(result)"
   ],
   "id": "c38d1c6295305924",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½Žè´¨é‡å›žç­”\n",
      "content='The expression \"2 ðŸ¦œ 9\" seems to use a parrot emoji (ðŸ¦œ) in place of a mathematical operator. If you could clarify what operation you intend to represent with the parrot emoji, I would be happy to help you solve it!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 17, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CMYc7pECDPUAcwUFdS1hUE4FNimGP', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--d37d6503-d439-4661-96a5-221fd4088ad2-0' usage_metadata={'input_tokens': 17, 'output_tokens': 56, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "[HumanMessage(content='2 ðŸ¦œ 2', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='2 ðŸ¦œ 3', additional_kwargs={}, response_metadata={}), AIMessage(content='5', additional_kwargs={}, response_metadata={})]\n",
      "ä½¿ç”¨few_shotåŽçš„é«˜è´¨é‡å›žç­”\n",
      "content='11' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 60, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CMYc98lcRdxF8z2qGaHsnktLFhhdZ', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--ea900c47-6620-48f9-ad6f-e02b71f58996-0' usage_metadata={'input_tokens': 60, 'output_tokens': 2, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T13:17:47.029591Z",
     "start_time": "2025-10-08T13:17:40.819123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å®‰è£…å‘é‡æ•°æ®åº“\n",
    "! pip install chromadb==0.4.15 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ],
   "id": "ab10aaf929e86c29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: chromadb==0.4.15 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (0.4.15)\n",
      "Requirement already satisfied: requests>=2.28 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (2.32.5)\n",
      "Requirement already satisfied: pydantic>=1.9 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (2.11.9)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (0.118.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (0.37.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (6.7.6)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (4.15.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (3.8.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (1.75.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (0.19.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (8.5.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from chromadb==0.4.15) (2.3.3)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.15) (0.48.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2025.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (6.0.3)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.41.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (0.10)\n",
      "Requirement already satisfied: coloredlogs in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (25.9.23)\n",
      "Requirement already satisfied: packaging in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (24.2)\n",
      "Requirement already satisfied: protobuf in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (6.32.1)\n",
      "Requirement already satisfied: sympy in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.15) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb==0.4.15) (0.58b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.15) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.15) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb==0.4.15) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb==0.4.15) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb==0.4.15) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from requests>=2.28->chromadb==0.4.15) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from requests>=2.28->chromadb==0.4.15) (3.10)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb==0.4.15) (0.35.3)\n",
      "Requirement already satisfied: colorama in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from tqdm>=4.65.0->chromadb==0.4.15) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.15) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.15) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.15) (14.1.0)\n",
      "Requirement already satisfied: h11>=0.8 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (0.16.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (1.1.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (15.0.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (4.9.1)\n",
      "Requirement already satisfied: filelock in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.15) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.15) (2025.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.15) (3.23.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (2.19.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from starlette<0.49.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.15) (4.11.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.15) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb==0.4.15) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.15) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.15) (1.3.1)\n",
      "Requirement already satisfied: pyreadline3 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.15) (3.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in e:\\github\\llm\\02langchain\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19013bc75bb353e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
